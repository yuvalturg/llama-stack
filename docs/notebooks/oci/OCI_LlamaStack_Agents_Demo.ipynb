{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df693aa",
   "metadata": {},
   "source": [
    "# Building AI Agents with OCI and Llama Stack\n",
    "\n",
    "This notebook demonstrates how to build Agents using Oracle Cloud Infrastructure (OCI) Generative AI models through Llama Stack.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Basic Agents** - Create agents with custom instructions\n",
    "2. **Multi-Turn Conversations** - Agents that maintain context across interactions\n",
    "3. **Agentic Workflows** - Chain multiple agent capabilities together\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Configure OCI credentials:**\n",
    "   - Set up `~/.oci/config` with your OCI credentials\n",
    "   - Set the `OCI_COMPARTMENT_OCID` environment variable\n",
    "   - Set the `OCI_REGION` environment variable\n",
    "   - Set the `OCI_AUTH_TYPE` environment variable with 'config_file'\n",
    "\n",
    "2. **Start Llama Stack server with OCI provider:**\n",
    "   ```bash\n",
    "   llama stack run /path/to/your_oci_config.yaml\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84fa9c7",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffff75b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: llama-stack-client in /Users/omara/oci/venv/lib/python3.12/site-packages (0.4.0a12)\n",
      "Requirement already satisfied: oci in /Users/omara/oci/venv/lib/python3.12/site-packages (2.164.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (4.12.0)\n",
      "Requirement already satisfied: click in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (8.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (1.9.0)\n",
      "Requirement already satisfied: fire in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (0.7.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (0.28.1)\n",
      "Requirement already satisfied: pandas in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (2.3.3)\n",
      "Requirement already satisfied: prompt-toolkit in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (3.0.52)\n",
      "Requirement already satisfied: pyaml in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (25.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (2.12.5)\n",
      "Requirement already satisfied: requests in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (2.32.5)\n",
      "Requirement already satisfied: rich in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (14.2.0)\n",
      "Requirement already satisfied: sniffio in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (1.3.1)\n",
      "Requirement already satisfied: termcolor in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (3.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/omara/oci/venv/lib/python3.12/site-packages (from llama-stack-client) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/omara/oci/venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->llama-stack-client) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/omara/oci/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->llama-stack-client) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/omara/oci/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->llama-stack-client) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/omara/oci/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama-stack-client) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/omara/oci/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/omara/oci/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/omara/oci/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->llama-stack-client) (0.4.2)\n",
      "Requirement already satisfied: cryptography<46.0.0,>=3.2.1 in /Users/omara/oci/venv/lib/python3.12/site-packages (from oci) (45.0.7)\n",
      "Requirement already satisfied: pyOpenSSL<=25.1.0,>=17.5.0 in /Users/omara/oci/venv/lib/python3.12/site-packages (from oci) (25.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.5.3 in /Users/omara/oci/venv/lib/python3.12/site-packages (from oci) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2016.10 in /Users/omara/oci/venv/lib/python3.12/site-packages (from oci) (2025.2)\n",
      "Requirement already satisfied: circuitbreaker<3.0.0,>=1.3.1 in /Users/omara/oci/venv/lib/python3.12/site-packages (from oci) (2.1.3)\n",
      "Requirement already satisfied: cffi>=1.14 in /Users/omara/oci/venv/lib/python3.12/site-packages (from cryptography<46.0.0,>=3.2.1->oci) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/omara/oci/venv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.5.3->oci) (1.17.0)\n",
      "Requirement already satisfied: pycparser in /Users/omara/oci/venv/lib/python3.12/site-packages (from cffi>=1.14->cryptography<46.0.0,>=3.2.1->oci) (2.23)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/omara/oci/venv/lib/python3.12/site-packages (from pandas->llama-stack-client) (2.3.5)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/omara/oci/venv/lib/python3.12/site-packages (from pandas->llama-stack-client) (2025.2)\n",
      "Requirement already satisfied: wcwidth in /Users/omara/oci/venv/lib/python3.12/site-packages (from prompt-toolkit->llama-stack-client) (0.2.14)\n",
      "Requirement already satisfied: PyYAML in /Users/omara/oci/venv/lib/python3.12/site-packages (from pyaml->llama-stack-client) (6.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/omara/oci/venv/lib/python3.12/site-packages (from requests->llama-stack-client) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/omara/oci/venv/lib/python3.12/site-packages (from requests->llama-stack-client) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/omara/oci/venv/lib/python3.12/site-packages (from rich->llama-stack-client) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/omara/oci/venv/lib/python3.12/site-packages (from rich->llama-stack-client) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/omara/oci/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack-client) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install llama-stack-client oci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edb7d27b-4259-4f9e-aef1-96857bbfaa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… llama-stack-client version: 0.4.0-alpha.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional: Configure custom venv path if needed\n",
    "import sys\n",
    "import os\n",
    "\n",
    "venv_path = os.getenv('LLAMA_STACK_VENV_PATH')\n",
    "if venv_path:\n",
    "    sys.path.insert(0, venv_path)\n",
    "    print(f\"âœ… Python path updated to use venv: {venv_path}\")\n",
    "\n",
    "# Verify llama-stack-client is installed\n",
    "try:\n",
    "    import llama_stack_client\n",
    "    print(f\"âœ… llama-stack-client version: {llama_stack_client.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  llama-stack-client not found. Please install with: pip install llama-stack-client\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29d385d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OCI_COMPARTMENT_OCID is set\n",
      "âœ… OCI_REGION is set\n",
      "âœ… OCI_AUTH_TYPE is set\n",
      "\n",
      "âœ… Connected to Llama Stack server\n",
      "\n",
      "ðŸ“‹ Available models:\n",
      "   â€¢ oci/google.gemini-2.5-flash\n",
      "   â€¢ oci/google.gemini-2.5-pro\n",
      "   â€¢ oci/google.gemini-2.5-flash-lite\n",
      "   â€¢ oci/xai.grok-4-fast-non-reasoning\n",
      "   â€¢ oci/xai.grok-4-fast-reasoning\n",
      "   â€¢ oci/xai.grok-code-fast-1\n",
      "   â€¢ oci/xai.grok-4\n",
      "   â€¢ oci/xai.grok-3-mini-fast\n",
      "   â€¢ oci/xai.grok-3-fast\n",
      "   â€¢ oci/xai.grok-3\n",
      "   â€¢ oci/xai.grok-3-mini\n",
      "\n",
      "ðŸŽ¯ Using model: oci/google.gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from llama_stack_client import LlamaStackClient, Agent\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Check required OCI environment variables\n",
    "os.environ[\"OCI_COMPARTMENT_OCID\"] = \"ocid1.tenancy.oc1..xxx\" # set it with your compartment id\n",
    "os.environ[\"OCI_REGION\"] = \"us-phoenix-1\" # change it to your default region\n",
    "os.environ[\"OCI_AUTH_TYPE\"] = \"config_file\"\n",
    "required_env_vars = {\n",
    "    \"OCI_COMPARTMENT_OCID\": \"export OCI_COMPARTMENT_OCID='ocid1.compartment.oc1..xxx'\",\n",
    "    \"OCI_REGION\": \"export OCI_REGION='us-phoenix-1'\",\n",
    "    \"OCI_AUTH_TYPE\": \"export OCI_AUTH_TYPE='config_file'\",\n",
    "}\n",
    "\n",
    "missing_vars = []\n",
    "for var, example in required_env_vars.items():\n",
    "    if not os.getenv(var):\n",
    "        missing_vars.append(f\"  â€¢ {var}: {example}\")\n",
    "    else:\n",
    "        print(f\"âœ… {var} is set\")\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"\\nâš ï¸  WARNING: Missing required environment variables:\")\n",
    "    for msg in missing_vars:\n",
    "        print(msg)\n",
    "\n",
    "# Initialize client\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "print(\"\\nâœ… Connected to Llama Stack server\")\n",
    "\n",
    "# Get available models\n",
    "models = client.models.list()\n",
    "if not models:\n",
    "    raise RuntimeError(\"No models available. Please check your Llama Stack server configuration.\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Available models:\")\n",
    "for model in models:\n",
    "    print(f\"   â€¢ {model.id}\")\n",
    "\n",
    "# Select primary model\n",
    "model_id = models[0].id\n",
    "print(f\"\\nðŸŽ¯ Using model: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15cb98",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Creating a Basic Agent\n",
    "\n",
    "Agents in Llama Stack are autonomous AI systems with:\n",
    "- **Instructions**: System-level guidance that shapes behavior\n",
    "- **Sessions**: Conversation contexts that maintain history\n",
    "- **Turns**: Individual interactions within a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c898f8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Session tracking utilities ready\n"
     ]
    }
   ],
   "source": [
    "# Session tracking and cleanup utilities\n",
    "all_sessions: list[str] = []\n",
    "\n",
    "def track_session(session_id: str) -> str:\n",
    "    \"\"\"Track a session for later cleanup.\"\"\"\n",
    "    all_sessions.append(session_id)\n",
    "    return session_id\n",
    "\n",
    "def cleanup_sessions(session_ids: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Clean up sessions after use.\n",
    "\n",
    "    Args:\n",
    "        session_ids: List of session IDs to delete\n",
    "    \"\"\"\n",
    "    if not session_ids:\n",
    "        print(\"No sessions to clean up.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ðŸ§¹ Cleaning up {len(session_ids)} session(s)...\")\n",
    "    for sid in session_ids:\n",
    "        try:\n",
    "            client.conversations.delete(conversation_id=sid)\n",
    "            print(f\"   âœ… Deleted session: {sid}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Could not delete session {sid}: {e}\")\n",
    "    print(\"âœ… Session cleanup complete\")\n",
    "\n",
    "print(\"âœ… Session tracking utilities ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b41a537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created OCI Expert Agent\n"
     ]
    }
   ],
   "source": [
    "# Create a specialized agent with custom instructions\n",
    "cloud_expert_agent = Agent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are an Oracle Cloud Infrastructure (OCI) expert assistant.\n",
    "You help users understand and work with OCI services including:\n",
    "- Compute instances and shapes\n",
    "- Networking (VCN, subnets, security lists)\n",
    "- Storage (Block, Object, File)\n",
    "- AI/ML services and GPU instances\n",
    "- Identity and Access Management (IAM)\n",
    "\n",
    "Always provide practical, actionable advice with examples when possible.\n",
    "If you're unsure about something, say so clearly.\"\"\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Created OCI Expert Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b805e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created session: conv_b4f4097126e0759eac387b6e898e487d3490f6654381933b\n"
     ]
    }
   ],
   "source": [
    "# Create a session for our agent\n",
    "session_id = cloud_expert_agent.create_session(session_name=\"oci_consultation\")\n",
    "track_session(session_id)\n",
    "print(f\"âœ… Created session: {session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbdb1ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function for chatting with agents\n",
    "\n",
    "def chat_with_agent(\n",
    "    agent: Agent,\n",
    "    session_id: str,\n",
    "    message: str,\n",
    "    stream: bool = True,\n",
    "    verbose: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Send a message to an agent and get the full response.\n",
    "    Properly handles both text responses and tool calls.\n",
    "\n",
    "    Args:\n",
    "        agent: The Agent instance to chat with\n",
    "        session_id: The session ID for the conversation\n",
    "        message: The user message to send\n",
    "        stream: Whether to stream the response (default: True)\n",
    "        verbose: Whether to print debug information (default: False)\n",
    "\n",
    "    Returns:\n",
    "        The agent's response text\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If agent, session_id, or message is invalid\n",
    "    \"\"\"\n",
    "    if not agent:\n",
    "        raise ValueError(\"Agent cannot be None\")\n",
    "    if not session_id:\n",
    "        raise ValueError(\"Session ID cannot be empty\")\n",
    "    if not message:\n",
    "        raise ValueError(\"Message cannot be empty\")\n",
    "\n",
    "    print(f\"\\nðŸ‘¤ User: {message}\\n\")\n",
    "    print(\"ðŸ¤– Agent: \", end='')\n",
    "\n",
    "    response = agent.create_turn(\n",
    "        session_id=session_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        stream=stream,\n",
    "    )\n",
    "\n",
    "    streamed_text = \"\"\n",
    "    tool_calls_made = []\n",
    "\n",
    "    for chunk in response:\n",
    "        event = chunk.event\n",
    "        event_type = event.event_type\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[DEBUG] Event type: {event_type}\")\n",
    "            print(f\"[DEBUG] Event: {event}\")\n",
    "\n",
    "        if event_type == \"step_started\":\n",
    "            if hasattr(event, 'step_type'):\n",
    "                if event.step_type == \"tool_execution\":\n",
    "                    print(\"\\n    Executing tool...\", end='')\n",
    "\n",
    "        elif event_type == \"step_progress\":\n",
    "            if hasattr(event, 'delta'):\n",
    "                if hasattr(event.delta, 'text') and event.delta.text:\n",
    "                    text = event.delta.text\n",
    "                    streamed_text += text\n",
    "                    print(text, end='', flush=True)\n",
    "\n",
    "        elif event_type == \"step_completed\":\n",
    "            if hasattr(event, 'step_details'):\n",
    "                step = event.step_details\n",
    "                if hasattr(step, 'tool_calls') and step.tool_calls:\n",
    "                    for tc in step.tool_calls:\n",
    "                        tool_name = tc.tool_name if hasattr(tc, 'tool_name') else 'unknown'\n",
    "                        tool_calls_made.append(tool_name)\n",
    "                        print(f\"\\n   âœ… Tool '{tool_name}' completed\")\n",
    "\n",
    "        elif event_type == \"turn_completed\":\n",
    "            if hasattr(event, 'turn') and hasattr(event.turn, 'output_message'):\n",
    "                output = event.turn.output_message\n",
    "                if hasattr(output, 'content') and output.content:\n",
    "                    if isinstance(output.content, list):\n",
    "                        for block in output.content:\n",
    "                            if hasattr(block, 'text') and block.text:\n",
    "                                if not streamed_text:\n",
    "                                    print(block.text)\n",
    "                                    streamed_text = block.text\n",
    "                    elif hasattr(output.content, 'text'):\n",
    "                        if not streamed_text:\n",
    "                            print(output.content.text)\n",
    "                            streamed_text = output.content.text\n",
    "            break\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if tool_calls_made:\n",
    "        print(f\"ðŸ”§ Tools used: {', '.join(tool_calls_made)}\")\n",
    "\n",
    "    return streamed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2e3ae06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¤ User: How do I choose between them for training a large language model?\n",
      "\n",
      "ðŸ¤– Agent: Training a Large Language Model (LLM) is a resource-intensive task that requires careful selection and configuration of OCI services. The right choice depends on your model's size, training data volume, budget, parallelism strategy, and desired level of infrastructure management.\n",
      "\n",
      "Here's a breakdown of OCI services and how to choose between them for LLM training:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Compute (GPUs) - The Brain of Your LLM Training**\n",
      "\n",
      "This is the most critical component. LLM training heavily relies on high-performance GPUs with ample VRAM and fast interconnects.\n",
      "\n",
      "*   **Option A: Bare Metal GPU Instances (BM.GPU.A100-v2)**\n",
      "    *   **What it is:** Dedicated physical servers with multiple NVIDIA A100 GPUs directly attached. The `BM.GPU.A100-v2` shape, for example, features **8x NVIDIA A100 80GB PCIe GPUs**. Crucially, these instances are provisioned within a **cluster network** fabric which enables **RDMA over Converged Ethernet (RoCE)**.\n",
      "    *   **Pros:**\n",
      "        *   **Highest Performance:** No hypervisor overhead; direct access to hardware.\n",
      "        *   **Excellent for Distributed Training:** RoCE provides ultra-low latency, high-bandwidth communication directly between GPUs across multiple instances in the same cluster network, which is essential for frameworks like DeepSpeed, Megatron-LM, or PyTorch Distributed.\n",
      "        *   **Maximum VRAM:** 80GB per GPU on the A100-v2 is critical for larger models and batch sizes.\n",
      "        *   **Consistent Performance:** Dedicated resources mean no noisy neighbor issues.\n",
      "    *   **Cons:**\n",
      "        *   **Higher Cost:** Most expensive option.\n",
      "        *   **Less Flexible:** You manage the entire OS and software stack.\n",
      "        *   **Less Scalable for Individual GPUs:** You're committing to a full bare-metal server (e.g., 8 GPUs).\n",
      "    *   **Best for:**\n",
      "        *   **Training LLMs from scratch (billions to trillions of parameters).**\n",
      "        *   **Massive distributed training jobs** where inter-node communication latency is a bottleneck.\n",
      "        *   Researchers and enterprises needing **absolute peak performance** and full control.\n",
      "\n",
      "*   **Option B: Virtual Machine (VM) GPU Instances (VM.GPU.A10.X, VM.GPU.A100.X, VM.GPU.RTX3090.X)**\n",
      "    *   **What it is:** Virtual machines with dedicated or shared NVIDIA GPUs.\n",
      "        *   `VM.GPU.A10.X`: Features 1-4 NVIDIA A10 GPUs (24GB VRAM each). A10 is great for inference, smaller model training, or fine-tuning.\n",
      "        *   `VM.GPU.A100.X`: Features 1-2 NVIDIA A100 GPUs (40GB VRAM each). A powerful option, but limited to 2 GPUs and doesn't get the bare metal's RDMA cluster network.\n",
      "        *   `VM.GPU.RTX3090.X`: Features 1-4 NVIDIA RTX 3090 GPUs (24GB VRAM each). A more cost-effective option for serious compute tasks, often used in smaller setups.\n",
      "    *   **Pros:**\n",
      "        *   **Flexibility and Granularity:** You can provision VMs with 1, 2, or 4 GPUs depending on the shape, allowing for more granular scaling.\n",
      "        *   **Cost-Effective for Smaller Scale:** Often more cost-effective than bare metal for fine-tuning, smaller models, or experimental training runs.\n",
      "        *   **Managed OS:** OCI handles the underlying hardware virtualization.\n",
      "    *   **Cons:**\n",
      "        *   **Lower Peak Performance than BM:** Hypervisor overhead, and lower interconnect bandwidth between GPUs (within the same VM).\n",
      "        *   **No RDMA Cluster Network:** Inter-node communication over standard TCP/IP, which is slower for multi-node distributed training compared to bare metal's RoCE.\n",
      "        *   **Lower Total VRAM per VM:** Max 2x A100 (40GB) or 4x A10 (24GB) per VM.\n",
      "    *   **Best for:**\n",
      "        *   **Fine-tuning existing LLMs.**\n",
      "        *   **Training smaller-scale LLMs (e.g., up to 7B or 13B parameters) on a single node.**\n",
      "        *   **Development, experimentation, and proof-of-concept work.**\n",
      "        *   **Inference serving.**\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Storage - Where Your Data and Models Reside**\n",
      "\n",
      "LLM training involves massive datasets and often requires frequent model checkpointing.\n",
      "\n",
      "*   **Option A: OCI Object Storage**\n",
      "    *   **What it is:** Highly scalable, durable, and cost-effective object storage.\n",
      "    *   **Pros:**\n",
      "        *   **Massive Capacity:** Nearly infinite storage for your training datasets.\n",
      "        *   **Cost-Effective:** Low cost per GB, especially for large volumes of data.\n",
      "        *   **Highly Durable and Available:** Data is replicated across multiple fault domains.\n",
      "        *   **API Access:** Easily accessible from your compute instances via SDKs, CLI, or S3-compatible APIs.\n",
      "        *   **Tiering:** Can transition less-frequently accessed data to archive storage.\n",
      "    *   **Cons:**\n",
      "        *   **Higher Latency for Random Access:** Not designed for file-system-like random read/write operations.\n",
      "        *   **Requires mounting or downloading:** You typically download datasets to local storage or stream them, rather than directly processing them from Object Storage.\n",
      "    *   **Best for:**\n",
      "        *   **Storing raw and pre-processed training datasets.**\n",
      "        *   **Archiving model checkpoints and final models.**\n",
      "        *   **Input data for distributed preprocessing jobs.**\n",
      "\n",
      "*   **Option B: OCI File Storage (NFS)**\n",
      "    *   **What it is:** Managed NFSv3 file system that can be mounted by multiple compute instances.\n",
      "    *   **Pros:**\n",
      "        *   **Shared Access:** Multiple GPU instances can simultaneously read and write to the same file system, ideal for shared code, logs, and checkpoints.\n",
      "        *   **Good Performance:** Provides good throughput and IOPS (can be scaled by increasing provisioned capacity).\n",
      "        *   **File System Semantics:** Familiar file system interface.\n",
      "    *   **Cons:**\n",
      "        *   **Higher Cost than Object Storage:** More expensive per GB.\n",
      "        *   **Performance Tiers:** You need to pre-provision capacity/performance, which might become a bottleneck for very high IOPS.\n",
      "        *   **Regional:** Tied to a specific Availability Domain or region.\n",
      "    *   **Best for:**\n",
      "        *   **Shared code repositories and scripts.**\n",
      "        *   **Storing smaller, frequently accessed datasets that require shared access.**\n",
      "        *   **Saving model checkpoints iteratively during training** (especially if multiple nodes need to save/resume from shared states).\n",
      "        *   **Consolidating logs and metrics from distributed training.**\n",
      "\n",
      "*   **Option C: OCI Block Storage (Boot Volumes, Block Volumes)**\n",
      "    *   **What it is:** Network-attached block devices that can be attached to a single compute instance.\n",
      "    *   **Pros:**\n",
      "        *   **High Performance:** Can be configured for high IOPS and throughput (especially \"High Performance\" tier).\n",
      "        *   **Durable and Persistent:** Data persists even if the instance is terminated.\n",
      "        *   **Flexible Sizing:** Can be resized.\n",
      "    *   **Cons:**\n",
      "        *   **Single Attach:** Can only be attached to one instance at a time.\n",
      "        *   **More Expensive than Object Storage:** Pricier per GB than Object Storage.\n",
      "    *   **Best for:**\n",
      "        *   **Operating System (boot volume).**\n",
      "        *   **Providing high-performance local scratch space for temporary data, libraries, and pip/conda environments for a *single* GPU instance.**\n",
      "        *   **Caching frequently accessed parts of a dataset copied from Object Storage.**\n",
      "\n",
      "*   **Option D: Local NVMe SSDs (on some GPU shapes)**\n",
      "    *   **What it is:** High-speed Non-Volatile Memory Express (NVMe) Solid State Drives directly attached to the compute host.\n",
      "    *   **Pros:**\n",
      "        *   **Extremely High Performance:** The fastest local storage option, with very low latency and high IOPS/throughput.\n",
      "        *   **Suitable for Scratch Space:** Ideal for temporary files, pre-processing large datasets before feeding them to the GPU.\n",
      "    *   **Cons:**\n",
      "        *   **Ephemeral:** **Data is lost when the instance is terminated.** This is a critical point.\n",
      "        *   **Limited Capacity:** Usually 36TB total on A100 BM.\n",
      "    *   **Best for:**\n",
      "        *   **Temporary scratch storage for data pre-processing.**\n",
      "        *   **Caching hot portions of very large datasets to maximize GPU utilization.**\n",
      "        *   **Storing intermediate results that don't need to persist.**\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Networking - The Communication Backbone**\n",
      "\n",
      "For distributed training, network latency and bandwidth are as important as GPU power.\n",
      "\n",
      "*   **VCN, Subnets, Security Lists, Network Security Groups (NSGs):** These are standard for any OCI deployment and provide network isolation, routing, and access control. You'll use them to define your training environment.\n",
      "*   **Crucial for Distributed Training: RDMA over Converged Ethernet (RoCE)**\n",
      "    *   **What it is:** A very high-speed, low-latency network protocol that allows direct memory access between servers, bypassing the CPU for network operations.\n",
      "    *   **Why it's important:** For multi-node GPU training, especially with frameworks like NVIDIA NCCL or MPI, RoCE allows GPUs on different machines to communicate almost as if they were on the same machine. This significantly reduces the overhead of gradient synchronization and data exchange.\n",
      "    *   **Where to find it:** Exclusively available on **Bare Metal GPU instances (like BM.GPU.A100-v2)** when provisioned within a **Cluster Network**. OCI's cluster networks provide this specialized, high-performance fabric.\n",
      "    *   **Recommendation:** If you plan on training an LLM that requires partitioning across multiple physical machines (e.g., any model with hundreds of billions or trillions of parameters), **bare metal A100 instances in a Cluster Network with RoCE are non-negotiable.** Without it, inter-node communication will become a massive bottleneck.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. AI/ML Platforms (Managed Services) - Streamlining Your Workflow**\n",
      "\n",
      "While you can run everything on plain compute instances, OCI offers managed services that simplify MLOps.\n",
      "\n",
      "*   **Option A: OCI Data Science**\n",
      "    *   **What it is:** A fully managed platform for data scientists to build, train, and deploy machine learning models. It includes:\n",
      "        *   **Managed Jupyter Notebooks:** Integrated environment for development.\n",
      "        *   **Training Sessions (Jobs):** Run your training scripts on isolated environments, leveraging GPU instances (VM and Bare Metal types).\n",
      "        *   **Model Catalog:** Store, manage, and version your trained models.\n",
      "        *   **ML Pipelines:** Orchestrate multi-step MLOps workflows.\n",
      "        *   **Experiment Tracking:** Logs metrics and parameters.\n",
      "    *   **Pros:**\n",
      "        *   **Reduced Operational Overhead:** OCI manages the infrastructure, patching, and scaling of the platform itself.\n",
      "        *   **Integrated Workflow:** Provides a cohesive environment from data exploration to model deployment.\n",
      "        *   **GPU Integration:** Easily provision and manage GPU instances (including BM.GPU.A100-v2 for jobs) within the platform.\n",
      "        *   **Pre-built Conda Environments:** Accelerates setup with optimized ML libraries.\n",
      "    *   **Cons:**\n",
      "        *   **Abstraction Layer:** While beneficial, some users might prefer full SSH access and control over the OS.\n",
      "        *   **Learning Curve:** Requires learning the OCI Data Science service's specific tools and APIs.\n",
      "    *   **Best for:**\n",
      "        *   **Most LLM training projects, especially those requiring MLOps discipline.**\n",
      "        *   **Teams that want to focus on model development rather than infrastructure.**\n",
      "        *   **Managing multiple training runs, experiments, and model versions.**\n",
      "\n",
      "*   **Option B: OCI Generative AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation - the agent maintains context!\n",
    "response = chat_with_agent(\n",
    "    cloud_expert_agent,\n",
    "    session_id,\n",
    "    \"How do I choose between them for training a large language model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a698e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Specialized Agent Personas\n",
    "\n",
    "You can create multiple agents with different expertise for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b90a0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created specialized agents:\n",
      "   â€¢ Security Agent\n",
      "   â€¢ DevOps Agent\n"
     ]
    }
   ],
   "source": [
    "# Create a security-focused agent\n",
    "security_agent = Agent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are an OCI Security Specialist. Your expertise includes:\n",
    "- Identity and Access Management (IAM) policies\n",
    "- Network security (Security Lists, NSGs, WAF)\n",
    "- Encryption (Vault, Key Management)\n",
    "- Security Zones and Cloud Guard\n",
    "- Compliance and auditing\n",
    "\n",
    "Always emphasize security best practices and zero-trust principles.\n",
    "Warn users about potential security risks in their configurations.\n",
    "Provide specific policy examples when relevant.\"\"\",\n",
    ")\n",
    "\n",
    "# Create a DevOps agent\n",
    "devops_agent = Agent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are an OCI DevOps Engineer. Your expertise includes:\n",
    "- CI/CD pipelines with OCI DevOps\n",
    "- Container services (OKE, Container Instances)\n",
    "- Infrastructure as Code (Terraform, Resource Manager)\n",
    "- Monitoring and observability\n",
    "- Automation and scripting with OCI CLI/SDK\n",
    "\n",
    "Provide practical, automation-focused advice.\n",
    "Include code snippets and CLI commands when helpful.\n",
    "Emphasize repeatability and infrastructure as code.\"\"\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Created specialized agents:\")\n",
    "print(\"   â€¢ Security Agent\")\n",
    "print(\"   â€¢ DevOps Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c58a380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sessions created\n"
     ]
    }
   ],
   "source": [
    "# Create sessions for each agent\n",
    "security_session = security_agent.create_session(session_name=\"security_review\")\n",
    "track_session(security_session)\n",
    "devops_session = devops_agent.create_session(session_name=\"devops_planning\")\n",
    "track_session(devops_session)\n",
    "\n",
    "print(\"âœ… Sessions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91391ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECURITY AGENT\n",
      "================================================================================\n",
      "\n",
      "ðŸ‘¤ User: What IAM policies do I need to set up for a team that manages AI workloads?\n",
      "\n",
      "ðŸ¤– Agent: Managing AI workloads in OCI involves a diverse set of services, from core machine learning platforms to underlying compute, storage, and networking. Implementing robust IAM policies is crucial to maintain security, adhere to zero-trust principles, and ensure the team has precisely the access they need, and no more.\n",
      "\n",
      "### Zero-Trust Foundation\n",
      "\n",
      "Before defining specific policies, establish a strong foundation:\n",
      "\n",
      "1.  **Dedicated Compartment**: Create a dedicated compartment (e.g., `ai-workloads`) for all AI-related resources. This simplifies policy management and isolation.\n",
      "2.  **Dedicated IAM Group**: Create an IAM group for your AI team (e.g., `AI_Workloads_Team`). All policies will be applied to this group.\n",
      "3.  **Principle of Least Privilege**: Start with minimal access and add permissions only as needed. Avoid `manage ALL resources` at the tenancy level.\n",
      "\n",
      "---\n",
      "\n",
      "### IAM Policies for the AI Workloads Team\n",
      "\n",
      "Here's a breakdown of common services used by AI teams and the recommended IAM policies. Replace `AI_Workloads_Team` with your group name and `AI_Workloads_Compartment` with your compartment name.\n",
      "\n",
      "#### 1. Core AI/ML Service Permissions\n",
      "\n",
      "These are the primary services an AI team will interact with for model development, training, and deployment.\n",
      "\n",
      "*   **OCI Data Science (ADS)**: For notebooks, projects, model catalogs, jobs, and model deployments.\n",
      "    ```\n",
      "    Allow group AI_Workloads_Team to manage data-science-family in compartment AI_Workloads_Compartment\n",
      "    ```\n",
      "    *   **Why**: This is the core platform. `manage` allows creation, deletion, and updates of all Data Science resources.\n",
      "    *   **Warning**: This grants extensive control over Data Science resources. Consider `use` or `read` if the team's role is more restricted (e.g., only consuming existing models).\n",
      "\n",
      "*   **OCI AI Services (Cognitive Services)**: For pre-built AI capabilities like Vision, Language, Speech, Anomaly Detection, Digital Assistant.\n",
      "    ```\n",
      "    Allow group AI_Workloads_Team to manage ai-service-family in compartment AI_Workloads_Compartment\n",
      "    ```\n",
      "    *   **Why**: Allows the team to create and manage resources for these pre-trained services.\n",
      "\n",
      "*   **OCI Generative AI**: For accessing and fine-tuning OCI's generative models.\n",
      "    ```\n",
      "    Allow group AI_Workloads_Team to manage generative-ai-family in compartment AI_Workloads_Compartment\n",
      "    ```\n",
      "    *   **Why**: To provision and interact with Generative AI models.\n",
      "\n",
      "#### 2. Supporting Infrastructure Permissions\n",
      "\n",
      "AI workloads rely heavily on various OCI infrastructure services.\n",
      "\n",
      "*   **Compute (VMs/GPUs for Training/Inference)**:\n",
      "    ```\n",
      "    Allow group AI_Workloads_Team to manage instance-family in compartment AI_Workloads_Compartment\n",
      "    Allow group AI_Workloads_Team to manage volume-family in compartment AI_Workloads_Compartment\n",
      "    Allow group AI_Workloads_Team to manage boot-volume-family in compartment AI_Workloads_Compartment\n",
      "    Allow group AI_Workloads_Team to manage image-family in compartment AI_Workloads_Compartment\n",
      "    ```\n",
      "    *   **Why**: To launch and manage compute instances (VMs, BM, GPU instances) for custom training or inference, attach/detach block volumes, and manage custom images.\n",
      "    *   **Security Risk**: `manage instance-family` allows provisioning powerful compute, including GPU instance shapes, which can incur significant costs. Ensure budget controls are in place.\n",
      "\n",
      "*   **Object Storage (Datasets, Models, Logs)**:\n",
      "    ```\n",
      "    Allow group AI_Workloads_Team to manage object-family in compartment AI_Workloads_Compartment\n",
      "    ```\n",
      "    *   **Why**: Essential for storing raw data, processed datasets, model artifacts, training logs, and inference results.\n",
      "    *   **(CRITICAL) Resource Principals**: For OCI Data Science notebooks/jobs to access Object Storage buckets *without* requiring users to manage credentials, you MUST grant permissions to the **service itself**:\n",
      "        ```\n",
      "        Allow service datascience to manage object-family in compartment AI_Workloads_Compartment\n",
      "        ```\n",
      "        This ensures secure, credential-less access for the Data Science service to your buckets.\n",
      "\n",
      "*   **File Storage (Shared Notebooks, Large Datasets)**:\n",
      "    ```\n",
      "    Allow group AI_Workloads_Team to manage file-storage-family in compartment AI_Workloads_Compartment\n",
      "    ```\n",
      "    *   **Why**: Provides shared network file systems, useful for collaborative notebook environments or large datasets needing a POSIX-compliant interface.\n",
      "\n",
      "*   **Networking (VCN, Sub\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask the security agent\n",
    "print(\"=\" * 80)\n",
    "print(\"SECURITY AGENT\")\n",
    "print(\"=\" * 80)\n",
    "response = chat_with_agent(\n",
    "    security_agent,\n",
    "    security_session,\n",
    "    \"What IAM policies do I need to set up for a team that manages AI workloads?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22c4369d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEVOPS AGENT\n",
      "================================================================================\n",
      "\n",
      "ðŸ‘¤ User: How do I set up a CI/CD pipeline for deploying ML models on OCI?\n",
      "\n",
      "ðŸ¤– Agent: As an OCI DevOps Engineer, setting up a CI/CD pipeline for ML model deployment on OCI is a core task that leverages several services to ensure repeatability, reliability, and automation. The goal is to take trained ML models, package them, and deploy them to an inference endpoint with minimal manual intervention.\n",
      "\n",
      "Here's a comprehensive guide, focusing on automation, IaC, and OCI services:\n",
      "\n",
      "## CI/CD Pipeline for ML Model Deployment on OCI\n",
      "\n",
      "### Core Principles\n",
      "\n",
      "1.  **Infrastructure as Code (IaC):** Define your OCI resources (VCNs, IAM, OKE clusters, Container Registry, Data Science Projects, etc.) using Terraform. This ensures your infrastructure is version-controlled, repeatable, and consistent across environments.\n",
      "2.  **Environment Parity:** Strive for identical (or near-identical) environments for development, staging, and production.\n",
      "3.  **Automation First:** Every step that can be automated, should be. Use OCI DevOps build and deploy pipelines.\n",
      "4.  **Version Control Everything:** Code (model training, inference API), Dockerfiles, infrastructure definitions, pipeline definitions (`build_spec.yaml`, `deployment_spec.yaml`).\n",
      "5.  **Observability:** Integrate monitoring and logging from the start to quickly identify and troubleshoot issues.\n",
      "\n",
      "### Key OCI Services Utilized\n",
      "\n",
      "*   **OCI DevOps:** For CI/CD orchestration (Code Repositories, Build Pipelines, Deploy Pipelines, Artifacts, Environments).\n",
      "*   **OCI Data Science:** For model training (Jobs, Notebook Sessions), model storage (Model Catalog), and managed model deployments (inference endpoints). This is often the preferred choice for simplicity and integration with ML lifecycle management.\n",
      "*   **OCI Container Registry (OCIR):** To store Docker images for your inference API.\n",
      "*   **OCI Object Storage:** To store raw model artifacts, datasets, and experiment results.\n",
      "*   **OCI Container Instances / OKE (Optional):** Alternative deployment targets for more custom or scaled inference services.\n",
      "*   **OCI Resource Manager (Terraform):** For IaC provisioning.\n",
      "*   **OCI Monitoring & Logging:** For observability of the deployed model and infrastructure.\n",
      "*   **OCI Notifications:** For alerting on pipeline status or model performance.\n",
      "\n",
      "### Architectural Overview\n",
      "\n",
      "```\n",
      "[Developer] --- Commits Code & Dockerfile ---> [OCI DevOps Code Repository]\n",
      "                                 |\n",
      "                                 V\n",
      "         [OCI DevOps Build Pipeline] --- Triggers ---> [Container Registry (OCIR)]\n",
      "          (Builds Docker Image, Runs Tests, Uploads Model Artifact to OCI DS Model Catalog/Object Storage)\n",
      "                                 |\n",
      "                                 V\n",
      "       [OCI DevOps Artifacts] <--- (Stores image reference, model OCID)\n",
      "                                 |\n",
      "                                 V\n",
      "         [OCI DevOps Deploy Pipeline] --- Triggers ---> [OCI Data Science (Model Deployment)\n",
      "          (Deploys/Updates Model Deployment, Tests)       OR OCI Container Instances\n",
      "                                                          OR OKE]\n",
      "                                 |\n",
      "                                 V\n",
      "                          [OCI Monitoring & Logging]\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Step-by-Step Implementation Guide\n",
      "\n",
      "#### 1. Prerequisites (IAM & VCN)\n",
      "\n",
      "Ensure your OCI tenancy has the necessary networking (VCN, subnets, security lists) and IAM policies for OCI DevOps, Data Science, Object Storage, OCIR, and other services to interact.\n",
      "\n",
      "*   **IaC First with Terraform:**\n",
      "\n",
      "    ```terraform\n",
      "    # Example: IAM Policy for DevOps accessing Data Science and other resources\n",
      "    resource \"oci_identity_policy\" \"devops_access_policy\" {\n",
      "      provider        = oci.home\n",
      "      compartment_id  = var.compartment_ocid\n",
      "      name            = \"DevOpsServiceAccessPolicy\"\n",
      "      description     = \"Allows OCI DevOps to manage Data Science, Object Storage, and OCIR.\"\n",
      "      statements = [\n",
      "        \"Allow group OCI-DEVOPS-OPERATORS to manage repos in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow group OCI-DEVOPS-OPERATORS to manage build-pipelines in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow group OCI-DEVOPS-OPERATORS to manage deploy-pipelines in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow group OCI-DEVOPS-OPERATORS to manage artifact-repositories in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow service devops to manage repos in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow service devops to manage build-pipelines in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow service devops to manage deploy-pipelines in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow service devops to manage artifact-repositories in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow service devops to use ons-topics in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow service devops to use metrics in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow service devops to manage all-artifacts in compartment id id ${var.compartment_ocid}\",\n",
      "\n",
      "        # Required for OCI Data Science interactions\n",
      "        \"Allow service devops to use data-science-family in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow service devops to use object-family in compartment id ${var.compartment_ocid}\",\n",
      "        \"Allow service devops to manage repos in tenancy\", # Needed for OCIR\n",
      "        \"Allow service devops to manage object-family in tenancy\", # Needed for OCIR\n",
      "        \"Allow service devops to inspect compartments in tenancy\"\n",
      "        # ... add policies for Container Instances, OKE if used\n",
      "      ]\n",
      "    }\n",
      "    ```\n",
      "\n",
      "#### 2. OCI DevOps Project Setup (IaC)\n",
      "\n",
      "Define your OCI DevOps project, code repositories, artifact repositories, and environments using Terraform.\n",
      "\n",
      "```terraform\n",
      "# main.tf for OCI DevOps Project\n",
      "resource \"oci_devops_project\" \"ml_deploy_project\" {\n",
      "  compartment_id = var.compartment_ocid\n",
      "  name           = \"ML-Model-Deployment-Project\"\n",
      "  description    = \"DevOps project for CI/CD of ML models\"\n",
      "  # notification_config {\n",
      "  #   topic_id = oci_ons_topic.devops_notifications.id\n",
      "  # }\n",
      "}\n",
      "\n",
      "resource \"oci_devops_repository\" \"model_code_repo\" {\n",
      "  project_id   = oci_devops_project.ml_deploy_project.id\n",
      "  name         = \"ml-inference-code\"\n",
      "  repository_type = \"OCI_CODE_REPO\"\n",
      "  description  = \"Code for ML model training and inference API\"\n",
      "}\n",
      "\n",
      "resource \"oci_devops_artifact_repository\" \"model_artifacts\" {\n",
      "  project_id = oci_devops_project.ml_deploy_project.id\n",
      "  name       = \"ml-model-artifacts\"\n",
      "  repository_type = \"GENERIC\" # To store model OCID, image tag etc.\n",
      "  description = \"Repository for storing model artifacts (OCIDs, image tags).\"\n",
      "}\n",
      "\n",
      "# Example of an OCI Data Science Deployment Environment\n",
      "resource \"oci_devops_deployment_environment\" \"ds_prod_environment\" {\n",
      "  compartment_id = var.compartment_ocid\n",
      "  project_id     = oci_devops_project.ml_deploy_project.id\n",
      "  name           = \"Production-DS-Model-Deployment\"\n",
      "  description    = \"Production environment for Data Science Model Deployments.\"\n",
      "  deploy_environment_type = \"GENERIC_CONFIG\" # Can be KUBERNETES_CLUSTER, INSTANCE_GROUP as well\n",
      "}\n",
      "\n",
      "# CLI equivalent for creating a project (after Terraform provisioned dependencies)\n",
      "# oci devops project create --compartment-id <COMPARTMENT_OCID> --name \"ML-Model-Deployment-Project\"\n",
      "# oci devops repository create --project-id <PROJECT_OCID> --name \"ml-inference-code\" --repository-type OCI_CODE_REPO\n",
      "```\n",
      "\n",
      "#### 3. Source Code Management\n",
      "\n",
      "Store your ML model training script, inference API code (e.g., Flask/FastAPI), Dockerfile, and pipeline definition files (`build_spec.yaml`, `deployment_spec.yaml`) in the OCI DevOps Code Repository.\n",
      "\n",
      "```bash\n",
      "# Clone the newly created OCI DevOps Code Repository\n",
      "oci devops repository get --repository-id <REPOSITORY_OCID> --query \"data.sshUrl\" --raw-output\n",
      "# git clone <ssh_url_from_above>\n",
      "\n",
      "# Example directory structure:\n",
      "# .\n",
      "# â”œâ”€â”€ .devops/\n",
      "# â”‚   â”œâ”€â”€ build_spec.yaml\n",
      "# â”‚   â””â”€â”€ deploy_spec.yaml\n",
      "# â”œâ”€â”€ app/\n",
      "# â”‚   â”œâ”€â”€ main.py        # FastAPI/Flask inference app\n",
      "# â”‚   â””â”€â”€ requirements.txt\n",
      "# â”œâ”€â”€ model/\n",
      "# â”‚   â””â”€â”€ train.py       # Model training script (produces model.pkl)\n",
      "# â”œâ”€â”€ Dockerfile         # Dockerfile for inference app\n",
      "# â””â”€â”€ README.md\n",
      "```\n",
      "\n",
      "#### 4. CI Pipeline (Build and Package)\n",
      "\n",
      "The CI pipeline will:\n",
      "1.  Check out code.\n",
      "2.  (Optional but recommended for full MLOps): Trigger an OCI Data Science Job for model training and version management.\n",
      "3.  Build a Docker image for the inference API.\n",
      "4.  Push the Docker image to OCIR.\n",
      "5.  Upload the trained model artifact to OCI Object Storage or OCI Data Science Model Catalog.\n",
      "6.  Pass relevant artifacts (image tag, model OCID) to the CD pipeline.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask the DevOps agent the same general area but different focus\n",
    "print(\"=\" * 80)\n",
    "print(\"DEVOPS AGENT\")\n",
    "print(\"=\" * 80)\n",
    "response = chat_with_agent(\n",
    "    devops_agent,\n",
    "    devops_session,\n",
    "    \"How do I set up a CI/CD pipeline for deploying ML models on OCI?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449ae44",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Multi-Agent Collaboration Pattern\n",
    "\n",
    "For complex tasks, you can coordinate multiple agents to work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "439f42f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Multi-agent consultation function ready\n"
     ]
    }
   ],
   "source": [
    "def multi_agent_consultation(question: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get perspectives from multiple specialized agents on a question.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" MULTI-AGENT CONSULTATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n Question: {question}\\n\")\n",
    "\n",
    "    responses = {}\n",
    "\n",
    "    # Get security perspective\n",
    "    print(\"\\n SECURITY PERSPECTIVE:\")\n",
    "    print(\"-\" * 40)\n",
    "    sec_session = security_agent.create_session(session_name=\"consultation_security\")\n",
    "    track_session(sec_session)\n",
    "    response = security_agent.create_turn(\n",
    "        session_id=sec_session,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"From a security perspective: {question}\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "    sec_response = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.event.event_type == \"turn_completed\":\n",
    "            sec_response = chunk.event.final_text\n",
    "            break\n",
    "        elif chunk.event.event_type == \"step_progress\":\n",
    "            if hasattr(chunk.event.delta, 'text'):\n",
    "                print(chunk.event.delta.text, end='', flush=True)\n",
    "    responses['security'] = sec_response\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Get DevOps perspective\n",
    "    print(\"\\n DEVOPS PERSPECTIVE:\")\n",
    "    print(\"-\" * 40)\n",
    "    devops_sess = devops_agent.create_session(session_name=\"consultation_devops\")\n",
    "    track_session(devops_sess)\n",
    "    response = devops_agent.create_turn(\n",
    "        session_id=devops_sess,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"From a DevOps perspective: {question}\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "    devops_response = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.event.event_type == \"turn_completed\":\n",
    "            devops_response = chunk.event.final_text\n",
    "            break\n",
    "        elif chunk.event.event_type == \"step_progress\":\n",
    "            if hasattr(chunk.event.delta, 'text'):\n",
    "                print(chunk.event.delta.text, end='', flush=True)\n",
    "    responses['devops'] = devops_response\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return responses\n",
    "\n",
    "print(\"âœ… Multi-agent consultation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c857d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " MULTI-AGENT CONSULTATION\n",
      "================================================================================\n",
      "\n",
      " Question: How should I architect a production ML inference service on OCI?\n",
      "\n",
      "\n",
      " SECURITY PERSPECTIVE:\n",
      "----------------------------------------\n",
      "Architecting a production ML inference service on OCI from a security perspective requires a multi-layered, defense-in-depth approach, heavily emphasizing **Zero-Trust principles**. The goal is to secure every component, from network ingress to data storage, and ensure continuous monitoring and auditing.\n",
      "\n",
      "Here's a breakdown of how to\n",
      "architect your service securely on OCI:\n",
      "\n",
      "---\n",
      "\n",
      "## Zero-Trust Foundation\n",
      "\n",
      "Before diving into specifics, embed these Zero-Trust principles throughout your design:\n",
      "\n",
      "1.  **Verify Explicitly:** Authenticate and authorize *every* request, whether from inside or outside your network perimeter.\n",
      "2.  **Least Privilege Access:** Grant only the minimum necessary permissions to users, services, and applications at all times.\n",
      "3.  **Assume Breach:** Design your security controls with the expectation that attackers *will* eventually gain access. Focus on limiting blast radius, swift detection, and rapid response.\n",
      "4.  **Micro-segmentation:** Segment your network and applications into isolated zones, restricting communication between them to only what is strictly necessary.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Identity and Access Management (IAM)\n",
      "\n",
      "IAM is the bedrock of OCI security.\n",
      "\n",
      "*   **Principle of Least Privilege:**\n",
      "    *   **Users & Groups:** Create dedicated groups for ML Engineers, Data Scientists, Operations, and Auditors. Assign them only the necessary permissions required for their roles.\n",
      "    *   **Dynamic Groups (for Services):** Use dynamic groups for OCI compute instances, OKE nodes, or Functions to grant *instance principals* or *resource principals* the exact permissions they need to interact with other OCI services (e.g., Object Storage, Vault, Logging). This avoids hardcoding credentials.\n",
      "    *   **Example Policy for Dynamic Group:**\n",
      "        ```\n",
      "        Allow dynamic-group <your-inference-compute-dynamic-group> to read object-family in compartment <your-model-storage-compartment> where target.bucket.name = '<your-model-bucket>'\n",
      "        Allow dynamic-group <your-inference-compute-dynamic-group> to use secret-family in compartment <your-secrets-compartment> where target.secret.name = '<your-model-access-token-secret>'\n",
      "        ```\n",
      "*   **Strong Authentication:**\n",
      "    *   **MFA:** Enforce Multi-Factor Authentication for all human users accessing the OCI Console.\n",
      "    *   **API Keys/Auth Tokens:** For programmatic access, use API keys with specific IAM policies. Rotate them regularly. Avoid using them directly on compute instances; prefer Instance Principals.\n",
      "*   **Authorization Scopes:** Define policies at the compartment level to ensure isolation. Use conditions in policies for fine-grained control (e.g., `where request.principal.name = '<specific-user>'`).\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Network Security\n",
      "\n",
      "Isolate and protect your ML inference service from the network layer up.\n",
      "\n",
      "*   **Virtual Cloud Network (VCN):**\n",
      "    *   **Dedicated Production VCN:** Use a separate VCN for your production environment. Avoid mixing prod, dev, and test in the same VCN.\n",
      "    *   **Private Subnets:** All core inference components (compute instances, OKE clusters, OCI Functions, private endpoints for databases/storage) *must* reside in private subnets.\n",
      "    *   **Public Subnets (Minimize):** Only place Load Balancers (if public) or WAF in public subnets, and only if absolutely necessary for external access.\n",
      "*   **Network Security Groups (NSGs) - *Preferred over Security Lists*:**\n",
      "    *   **Micro-segmentation:** Apply NSGs directly to individual resources (compute VMs, OKE pods, Load Balancers) for granular control.\n",
      "    *   **Inbound Rules:** Restrict inbound traffic to the absolute minimum necessary.\n",
      "        *   **Example:** Allow ingress on port 8000 (inference port) from the Load Balancer's NSG only.\n",
      "        *   **Example:** Allow ingress on port 22 (SSH) only from your Bastion Service's NSG or a designated Management subnet.\n",
      "    *   **Outbound Rules:** Restrict outbound traffic. Allow only necessary outbound connections (e.g., to Object Storage via a Service Gateway, Vault via a Private Endpoint, Logging/Monitoring endpoints). Block all other outbound to the internet.\n",
      "*   **Load Balancer (LB):**\n",
      "    *   **SSL/TLS Termination:** Terminate SSL/TLS at the Load Balancer to encrypt traffic between clients and the LB. Use strong ciphers and protocols.\n",
      "    *   **Private Load Balancer:** If your inference service is consumed internally (e.g., by other internal applications), use a private LB in a private subnet.\n",
      "    *   **Public Load Balancer:** If public facing, place it behind a WAF.\n",
      "*   **Web Application Firewall (WAF):**\n",
      "    *   **Essential for Public Endpoints:** If your ML inference endpoint is exposed to the public internet, use OCI WAF to protect against common web exploits (SQL Injection, Cross-Site Scripting, DDoS attacks, etc.).\n",
      "    *   **Rate Limiting:** Configure WAF to rate-limit requests to prevent abuse or denial-of-service attempts.\n",
      "*   **Service Gateway & Private Endpoints:**\n",
      "    *   **Service Gateway:** Enables private connectivity from your private subnets to OCI services (Object Storage, Vault) without traversing the public internet. This is *critical* for securely accessing your models and secrets.\n",
      "    *   **Private Endpoints:** For services like OCI Vault or Autonomous Database, use Private Endpoints to ensure direct, private connectivity within your VCN.\n",
      "*   **Bastion Service:**\n",
      "    *   **Secure Access:** Use OCI Bastion Service for secure, time-limited, and audited SSH access to instances in private subnets. **Never expose SSH directly to the internet.**\n",
      "    *   **Just-in-Time Access:** Configure bastion sessions with a short TTL (Time-To-Live).\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Data Security (Models, Input, Output)\n",
      "\n",
      "Protect your valuable models and sensitive inference data at rest and in transit.\n",
      "\n",
      "*   **Encryption at Rest:**\n",
      "    *   **Customer-Managed Keys (CMK) via Vault:** For Object Storage (model artifacts, input/output data), Block Volumes, and Databases, always prefer using Customer-Managed Keys (CMK) from OCI Vault instead of OCI-managed keys. This gives you full control over key lifecycle.\n",
      "    *   **OCI Vault:** Store all cryptographic keys (CMKs) and secrets (API keys, database credentials, model access tokens, service account keys) securely in OCI Vault.\n",
      "*   **Encryption in Transit:**\n",
      "    *   **TLS/SSL Everywhere:** Enforce HTTPS for all external communication to your inference service (WAF, Load Balancer).\n",
      "    *   **Internal TLS:** Use TLS/SSL for internal communication between microservices within your inference architecture, especially if data is sensitive. OCI services communicate via TLS by default.\n",
      "*   **Data Masking/Anonymization:**\n",
      "    *   If your inference input or output data contains sensitive Personally Identifiable Information (PII), implement data masking or anonymization techniques *before* it reaches the inference service or before it's stored.\n",
      "*   **Model Integrity:**\n",
      "    *   Store ML model artifacts in versioned Object Storage buckets. Implement policies to ensure only authorized users or services (via Instance Principals) can update or delete models. Consider hashing or digitally signing models to detect tampering.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Compute and Runtime Environment\n",
      "\n",
      "Secure the environment where your inference code runs.\n",
      "\n",
      "*   **Hardened Images:** Start with OCI Platform Images or build custom images based on a minimal, hardened OS. Regularly patch and update images. Consider using OCI Vulnerability Scanning Service for your custom images.\n",
      "*   **Patch Management:**\n",
      "    *   **OS Management Service:** Leverage OCI OS Management Service to ensure your compute instances are regularly patched and updated for security vulnerabilities.\n",
      "    *   **Container Updates:** If using OKE, keep Kubernetes versions and container images up-to-date.\n",
      "*   **Container Security (if using OKE/Containers):**\n",
      "    *   **OCI Container Registry Scanning:** Use OCI Container Registry's built-in vulnerability scanning for your Docker images.\n",
      "    *   **Image Signing:** Sign your container images to ensure authenticity and integrity.\n",
      "    *   **Runtime Security:** Run containers with a non-root user, apply resource limits, and use Kubernetes Network Policies for micro-segmentation at the pod level.\n",
      "*   **OCI Functions:**\n",
      "    *   Small attack surface due to ephemeral nature. Focus on input validation, secure configurations, and private subnet deployments.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Monitoring, Logging, and Auditing\n",
      "\n",
      "Visibility is key to detecting and responding to security incidents.\n",
      "\n",
      "*   **OCI Audit Logs:**\n",
      "    *   **Centralized Collection:** Audit logs track *all* API calls and OCI Console actions.\n",
      "    *   **Retention:** Configure audit log retention for at least 1 year (or longer, based on compliance requirements).\n",
      "    *   **Alerting:** Set up alarms for critical security events (e.g., policy changes, unauthorized access attempts, deletion of security resources).\n",
      "*   **OCI Logging:**\n",
      "    *   **Comprehensive Log Collection:** Ingest logs from all components: Load Balancer access logs, WAF logs, compute instance logs, OKE cluster logs, OCI Functions logs.\n",
      "    *   **Log Analytics:** Use OCI Logging Analytics for centralized log management, advanced searching, correlation, and threat detection.\n",
      "*   **OCI Monitoring:**\n",
      "    *   **Security Metrics & Alarms:** Monitor key security metrics (network traffic anomalies, unauthorized access attempts, resource modifications, high CPU on bastion hosts, etc.). Set up alarms to notify security operations.\n",
      "*   **Cloud Guard:**\n",
      "    *   **Proactive Threat Detection:** Enable OCI Cloud Guard to continuously monitor your OCI tenancy for security misconfigurations, insecure activities, and threats. It identifies policy violations and suggests remediations.\n",
      "    *   **Security Zones Integration:** When paired with Security Zones, Cloud Guard can enforce *preventative* security policies at creation time.\n",
      "*   **Vulnerability Scanning Service:**\n",
      "    *   Automate vulnerability scanning of your compute instances to identify and remediate security flaws in the operating system and installed applications.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. OCI Security Zones\n",
      "\n",
      "**Strongly recommended for production workloads.**\n",
      "\n",
      "*   **Preventative Security:** Security Zones enforce OCI security best practices *at the point of resource creation*.\n",
      "*   **Mandatory Policies:** Within a security zone compartment, resources must adhere to a strict set of security policies (e.g., all volumes must be encrypted with CMK, public access to storage is forbidden, resources cannot be moved out of the zone).\n",
      "*   **Example (Implied by Security Zone):** You cannot provision a public subnet or object storage bucket with public access within a Security Zone.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Disaster Recovery & Business Continuity (DR/BC)\n",
      "\n",
      "While not strictly security, DR/BC planning ensures the availability of your secure service.\n",
      "\n",
      "*   **High Availability (HA):** Architect your service across multiple Availability Domains (ADs) and Fault Domains (FDs) within a region for resilience against outages.\n",
      "*   **Regional Disaster Recovery:** Plan for cross-region disaster recovery for catastrophic regional failures, ideally with automated failover.\n",
      "*   **Secure Backups:** Regularly back up your models, configurations, and critical data to encrypted Object Storage, preferably in a separate DR region. Test your restore procedures.\n",
      "\n",
      "---\n",
      "\n",
      "By meticulously implementing these security controls and continuously monitoring your environment, you can build a robust and secure ML inference service on OCI that adheres to the highest security standards and Zero-Trust principles."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " DEVOPS PERSPECTIVE:\n",
      "----------------------------------------\n",
      "Architecting a production ML inference service on OCI from a DevOps perspective centers around automation, reliability, scalability, and maintainability. The goal is to treat your ML model and its serving infrastructure as a single, version-controlled entity, deployed and managed through codified processes.\n",
      "\n",
      "Hereâ€™s a comprehensive approach, prioritizing OCI-native services and best practices:\n",
      "\n",
      "## Core Principles from a DevOps Perspective\n",
      "\n",
      "1.  **Infrastructure as Code (IaC):** Everythingâ€”VCN, OKE clusters, API Gateways, load balancers, even monitoring configurationsâ€”should be defined in Terraform. This ensures repeatability, version control, and immutability.\n",
      "2.  **Containerization:** Package your ML model and its serving application (e.g., Flask/FastAPI) into immutable Docker images.\n",
      "3.  **CI/CD Automation:** Use OCI DevOps services to automate building, testing, deploying, and managing your inference service.\n",
      "4.  **Observability:** Implement robust logging, metrics, and alerting to understand service health and performance.\n",
      "5.  **Security First:** Integrate OCI IAM, Vault, and VCN security features from the outset.\n",
      "6.  **Scalability & High Availability:** Design for resilience and automatic scaling to handle varying load and ensure uninterrupted service.\n",
      "\n",
      "---\n",
      "\n",
      "## Recommended Architecture: OKE + API Gateway (Most Versatile)\n",
      "\n",
      "For production-grade ML inference, **Oracle Container Engine for Kubernetes (OKE)** combined with **OCI API Gateway** offers the best balance of flexibility, scalability, and control.\n",
      "\n",
      "### Conceptual Diagram\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    User(End User/Client) --> |HTTPS| APIGW(OCI API Gateway)\n",
      "    APIGW --> LB(OCI Load Balancer)\n",
      "    LB --> OKE(Oracle Container Engine for Kubernetes)\n",
      "    OKE -- deploys --> Pod(Inference Service Pods)\n",
      "    Pod -. fetches .- OBJ(OCI Object Storage: Model Artifacts)\n",
      "    Pod -- emits --> LOGS(OCI Logging)\n",
      "    Pod -- metrics --> MONITOR(OCI Monitoring)\n",
      "    MONITOR -- alerts --> NOTIF(OCI Notifications)\n",
      "    APIGW -- config --> VAULT(OCI Vault: API Keys)\n",
      "    OKE -- config --> VAULT\n",
      "    DEV(DevOps Engineer) -- builds/deploys --> OCI_DEVOPS(OCI DevOps Service)\n",
      "    OCI_DEVOPS -- builds --> OCI_REG(OCI Container Registry)\n",
      "    OCI_DEVOPS -- deploys/manages --> OKE\n",
      "    CODE(Git Repository: Code, Dockerfile, K8s Manifests, Terraform) --> OCI_DEVOPS\n",
      "    CODE --> DEV\n",
      "```\n",
      "\n",
      "### Key OCI Services Involved\n",
      "\n",
      "*   **Compute:**\n",
      "    *   **Oracle Container Engine for Kubernetes (OKE):** Runs your containerized inference service. Provides auto-scaling, self-healing, and declarative management.\n",
      "*   **Networking:**\n",
      "    *   **Virtual Cloud Network (VCN):** Your isolated private network.\n",
      "    *   **Subnets:** Public (for Load Balancer, API Gateway endpoints) and Private (for OKE worker nodes, databases).\n",
      "    *   **Network Security Groups (NSGs)/Security Lists:** Fine-grained firewall rules.\n",
      "    *   **Load Balancer (LBaaS):** Distributes incoming traffic to your OKE service.\n",
      "    *   **API Gateway:** A managed, high-performance HTTP(S) proxy. Crucial for rate limiting, authentication, API key management, and transformation before requests hit your inference service.\n",
      "*   **Storage:**\n",
      "    *   **Object Storage:** Stores your trained ML models, datasets, and other artifacts. Cheap, highly available, and easily integrated.\n",
      "    *   **Container Registry (OCIR):** Stores your Docker images for the inference service.\n",
      "*   **Observability:**\n",
      "    *   **Logging:** Unified logging for container standard output, infrastructure logs.\n",
      "    *   **Monitoring:** Collects metrics (CPU, Memory, Request Latency, Error Rates) from OKE and your application.\n",
      "    *   **Alarms & Notifications:** Trigger alerts based on monitoring thresholds (e.g., PagerDuty, Slack, Email).\n",
      "    *   **Service Connector Hub:** Connects logging and monitoring streams to other services (e.g., data lakes).\n",
      "*   **DevOps & CI/CD:**\n",
      "    *   **OCI DevOps:** Comprehensive CI/CD platform (Code Repositories, Build Pipelines, Deploy Pipelines, Artifacts).\n",
      "*   **Security:**\n",
      "    *   **IAM (Identity and Access Management):** Manages user authentication and authorization.\n",
      "    *   **Vault:** Securely stores sensitive information like API keys, database credentials, or model encryption keys.\n",
      "    *   **WAF (Web Application Firewall):** Optional, but recommended for advanced threat protection at the perimeter.\n",
      "\n",
      "---\n",
      "\n",
      "## DevOps Workflow & Implementation Steps\n",
      "\n",
      "### Phase 1: Foundation with Infrastructure as Code (Terraform)\n",
      "\n",
      "Define your entire infrastructure using Terraform.\n",
      "\n",
      "1.  **VCN & Networking:** Set up your VCN, subnets (at least one private for OKE worker nodes and one public for Load Balancer/API Gateway), Security Lists/NSGs.\n",
      "2.  **OKE Cluster:** Provision your Kubernetes cluster. Define node pools, shapes, and scaling parameters.\n",
      "3.  **Object Storage:** Create a bucket for storing ML models.\n",
      "4.  **API Gateway:** Configure the API Gateway with routes pointing to your OKE Load Balancer. Implement authentication (e.g., API keys validated via OCI Vault, or IAM policies).\n",
      "5.  **Vault:** Create a vault and secrets for sensitive information (e.g., API keys for client access, credentials for model downloads if private).\n",
      "6.  **IAM Policies:** Define least-privilege policies for OKE (to access Object Storage, push logs), DevOps service (to build, deploy), and your application's service account.\n",
      "\n",
      "**Terraform Snippet Example (High-Level):**\n",
      "\n",
      "```terraform\n",
      "# main.tf\n",
      "resource \"oci_core_vcn\" \"main_vcn\" {\n",
      "  cidr_block = \"10.0.0.0/16\"\n",
      "  # ...\n",
      "}\n",
      "\n",
      "resource \"oci_containerengine_cluster\" \"ml_inference_cluster\" {\n",
      "  vcn_id            = oci_core_vcn.main_vcn.id\n",
      "  kubernetes_version = \"v1.28.x\"\n",
      "  # ... node pools, network configuration\n",
      "}\n",
      "\n",
      "resource \"oci_objectstorage_bucket\" \"ml_models_bucket\" {\n",
      "  name          = \"ml_inference_models\"\n",
      "  namespace     = data.oci_objectstorage_namespace.this.namespace\n",
      "  compartment_id = var.compartment_ocid\n",
      "  # ...\n",
      "}\n",
      "\n",
      "resource \"oci_apigateway_gateway\" \"ml_api_gateway\" {\n",
      "  compartment_id = var.compartment_ocid\n",
      "  display_name   = \"MLInferenceAPIGateway\"\n",
      "  # ...\n",
      "}\n",
      "\n",
      "resource \"oci_apigateway_deployment\" \"ml_inference_deployment\" {\n",
      "  gateway_id  = oci_apigateway_gateway.ml_api_gateway.id\n",
      "  compartment_id = var.compartment_ocid\n",
      "  specification {\n",
      "    routes {\n",
      "      path = \"/inference\"\n",
      "      methods = [\"POST\"]\n",
      "      backend {\n",
      "        type = \"HTTP_BACKEND\"\n",
      "        url    = oci_load_balancer.ml_inference_lb.ip_address # Example assumes LB IP, in K8s it would be a headless service or ingress controller\n",
      "        # For OKE, you'd integrate with the K8s service LoadBalancer IP or Ingress Controller\n",
      "      }\n",
      "      # ... authentication, rate limiting policies\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "# Add IAM policies, Vault, etc.\n",
      "```\n",
      "\n",
      "### Phase 2: Application Development & Containerization\n",
      "\n",
      "1.  **Develop Inference Service:** Create a Python (e.g., FastAPI, Flask) or other language application that:\n",
      "    *   Loads the ML model (e.g., from Object Storage at startup or on first request).\n",
      "    *   Exposes a prediction endpoint (e.g., `/predict`).\n",
      "    *   Handles input data, performs inference, and returns results.\n",
      "    *   Includes logging (stdout/stderr) for OCI Logging.\n",
      "    *   Exposes Prometheus metrics endpoint (e.g., `/metrics`) for OCI Monitoring scraping (via OKE service mesh or custom setup).\n",
      "2.  **Dockerfile:** Create a Dockerfile to package your application and its dependencies.\n",
      "\n",
      "**Dockerfile Example:**\n",
      "\n",
      "```dockerfile\n",
      "# Dockerfile\n",
      "FROM python:3.9-slim-buster\n",
      "\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy model first to leverage Docker cache if models change less often than code\n",
      "COPY models/ /app/models/\n",
      "# If models are large and pulled from OCI Object Storage at runtime,\n",
      "# adjust this to only include a placeholder or dynamically fetch.\n",
      "\n",
      "COPY requirements.txt .\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "COPY . .\n",
      "\n",
      "# Expose the port your inference service listens on\n",
      "EXPOSE 8000\n",
      "\n",
      "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
      "```\n",
      "\n",
      "### Phase 3: CI/CD with OCI DevOps Service\n",
      "\n",
      "#### Git Repository\n",
      "\n",
      "Store your application code, Dockerfile, Kubernetes manifests, and Terraform files in an OCI Code Repository (or GitHub/GitLab integrated with OCI DevOps).\n",
      "\n",
      "#### **Build Pipeline:**\n",
      "\n",
      "Automates building your Docker image and pushing it to OCIR.\n",
      "\n",
      "1.  **Source Stage:** Clone your Git repository.\n",
      "2.  **Build Stage:**\n",
      "    *   Run unit tests.\n",
      "    *   Build Docker image: `docker build -t <region>.ocir.io/<namespace>/<repo-name>:<build-number> .`\n",
      "    *   Login to OCIR: `docker login <region>.ocir.io -u '<tenant>/<username>' -p 'token'`\n",
      "    *   Push to OCIR: `docker push <region>.ocir.io/<namespace>/<repo-name>:<build-number>`\n",
      "    *   **Output Artifact:** Create a **Generic Artifact** in OCI DevOps for your Kubernetes manifest (`deployment.yaml`) where the `image` tag is parameterized.\n",
      "\n",
      "**OCI DevOps Build Spec (build_spec.yaml):**\n",
      "\n",
      "```yaml\n",
      "# build_spec.yaml for OCI DevOps Build Pipeline\n",
      "version: 1.0\n",
      "timeoutInSeconds: 3600\n",
      "inputArtifacts:\n",
      "  - artifacstPath: ./\n",
      "    name: source_code\n",
      "outputArtifacts:\n",
      "  - name: kubernetes_manifest\n",
      "    type: BINARY\n",
      "    location: deployment.yaml # Assuming your K8s manifest is named this\n",
      "steps:\n",
      "  - type: Command\n",
      "    name: \"Install Dependencies\"\n",
      "    timeoutInSeconds: 600\n",
      "    command: |\n",
      "      pip install docker\n",
      "  - type: Command\n",
      "    name: \"Build and Push Docker Image\"\n",
      "    timeoutInSeconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run a multi-agent consultation\n",
    "results = multi_agent_consultation(\n",
    "    \"How should I architect a production ML inference service on OCI?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4893b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Agentic Workflow: Infrastructure Review\n",
    "\n",
    "Let's add an architect agent that reviews the responses of the security and devops agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a0bee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created Architect Agent\n"
     ]
    }
   ],
   "source": [
    "# Create an architect agent that synthesizes recommendations\n",
    "architect_agent = Agent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are a Senior OCI Solutions Architect. Your role is to:\n",
    "- Review infrastructure requirements and constraints\n",
    "- Synthesize input from security and DevOps perspectives\n",
    "- Provide comprehensive, balanced architectural recommendations\n",
    "- Consider cost, performance, security, and operational aspects\n",
    "- Create actionable implementation plans\n",
    "\n",
    "You provide executive-level summaries followed by detailed technical guidance.\"\"\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Created Architect Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b45531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Infrastructure review workflow ready\n"
     ]
    }
   ],
   "source": [
    "def infrastructure_review_workflow(requirements: str) -> dict:\n",
    "    \"\"\"\n",
    "    A multi-stage workflow for infrastructure review.\n",
    "\n",
    "    Stage 1: Security Analysis\n",
    "    Stage 2: DevOps Analysis\n",
    "    Stage 3: Architecture Synthesis\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"  INFRASTRUCTURE REVIEW WORKFLOW\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n Requirements:\\n{requirements}\\n\")\n",
    "\n",
    "    # Stage 1: Security Analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" STAGE 1: Security Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    sec_session = security_agent.create_session(session_name=\"review_security\")\n",
    "    track_session(sec_session)\n",
    "    sec_response = security_agent.create_turn(\n",
    "        session_id=sec_session,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Review these requirements for security considerations:\\n\\n{requirements}\\n\\nProvide key security requirements and concerns.\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "    security_analysis = \"\"\n",
    "    for chunk in sec_response:\n",
    "        if chunk.event.event_type == \"turn_completed\":\n",
    "            security_analysis = chunk.event.final_text\n",
    "            break\n",
    "        elif chunk.event.event_type == \"step_progress\":\n",
    "            if hasattr(chunk.event.delta, 'text'):\n",
    "                print(chunk.event.delta.text, end='', flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Stage 2: DevOps Analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" STAGE 2: DevOps Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    devops_sess = devops_agent.create_session(session_name=\"review_devops\")\n",
    "    track_session(devops_sess)\n",
    "    devops_response = devops_agent.create_turn(\n",
    "        session_id=devops_sess,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Review these requirements for DevOps implementation:\\n\\n{requirements}\\n\\nProvide key operational requirements and automation opportunities.\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "    devops_analysis = \"\"\n",
    "    for chunk in devops_response:\n",
    "        if chunk.event.event_type == \"turn_completed\":\n",
    "            devops_analysis = chunk.event.final_text\n",
    "            break\n",
    "        elif chunk.event.event_type == \"step_progress\":\n",
    "            if hasattr(chunk.event.delta, 'text'):\n",
    "                print(chunk.event.delta.text, end='', flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Stage 3: Architecture Synthesis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" STAGE 3: Architecture Synthesis\")\n",
    "    print(\"=\"*80)\n",
    "    arch_session = architect_agent.create_session(session_name=\"review_synthesis\")\n",
    "    track_session(arch_session)\n",
    "    synthesis_prompt = f\"\"\"Based on the following inputs, provide a comprehensive infrastructure recommendation:\n",
    "\n",
    "## Original Requirements:\n",
    "{requirements}\n",
    "\n",
    "## Security Analysis:\n",
    "{security_analysis}\n",
    "\n",
    "## DevOps Analysis:\n",
    "{devops_analysis}\n",
    "\n",
    "Provide a synthesized recommendation that addresses all concerns with a clear implementation plan.\"\"\"\n",
    "\n",
    "    arch_response = architect_agent.create_turn(\n",
    "        session_id=arch_session,\n",
    "        messages=[{\"role\": \"user\", \"content\": synthesis_prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    final_recommendation = \"\"\n",
    "    for chunk in arch_response:\n",
    "        if chunk.event.event_type == \"turn_completed\":\n",
    "            final_recommendation = chunk.event.final_text\n",
    "            break\n",
    "        elif chunk.event.event_type == \"step_progress\":\n",
    "            if hasattr(chunk.event.delta, 'text'):\n",
    "                print(chunk.event.delta.text, end='', flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… REVIEW COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return {\n",
    "        \"security_analysis\": security_analysis,\n",
    "        \"devops_analysis\": devops_analysis,\n",
    "        \"final_recommendation\": final_recommendation\n",
    "    }\n",
    "\n",
    "print(\"âœ… Infrastructure review workflow ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37c0f9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  INFRASTRUCTURE REVIEW WORKFLOW\n",
      "================================================================================\n",
      "\n",
      " Requirements:\n",
      "\n",
      "We need to deploy a real-time ML inference service with the following requirements:\n",
      "\n",
      "1. Handle 1000 requests per second at peak\n",
      "2. Maximum latency of 100ms for 99th percentile\n",
      "3. Use GPU-accelerated inference\n",
      "4. Support model updates without downtime\n",
      "5. Store inference logs for 90 days\n",
      "6. Operate in us-phoenix-1 region\n",
      "7. Budget: ~$10,000/month\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      " STAGE 1: Security Analysis\n",
      "================================================================================\n",
      "Deploying a real-time ML inference service on OCI with your specified requirements (high throughput, low latency, GPU acceleration, zero-downtime updates, log retention, budget) presents several critical security considerations. As an OCI Security Specialist, my focus will be on embedding security from the ground up, adhering strictly to Zero Trust principles, and emphasizing robust controls.\n",
      "\n",
      "Here are the key security requirements and concerns, broken down by domain:\n",
      "\n",
      "---\n",
      "\n",
      "### **Zero Trust Foundation**\n",
      "\n",
      "Before diving into specifics, it's crucial to establish a Zero Trust mindset:\n",
      "\n",
      "*   **Never Trust, Always Verify:** Assume no user, application, or service is inherently trustworthy, even within your VCN.\n",
      "*   **Least Privilege:** Grant only the minimum necessary permissions for any entity (user, group, instance, service).\n",
      "*   **Micro-segmentation:** Isolate components to limit blast radius in case of compromise.\n",
      "*   **End-to-End Encryption:** Encrypt data at rest and in transit.\n",
      "*   **Continuous Monitoring & Validation:** Regularly check security posture and enforce policies.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Identity and Access Management (IAM)**\n",
      "\n",
      "This is the cornerstone of your security posture.\n",
      "\n",
      "*   **Concern:** Unauthorized access to your inference service, models, logs, or underlying infrastructure could lead to data exfiltration, service disruption, or intellectual property theft.\n",
      "*   **Requirement:** Implement granular access controls for all human and programmatic entities.\n",
      "\n",
      "    *   **Strong Authentication:** Enforce Multi-Factor Authentication (MFA) for all administrative and privileged users.\n",
      "    *   **Least Privilege for Humans:**\n",
      "        *   Create specific IAM groups for ML engineers, MLOps, security auditing, and operations.\n",
      "        *   Grant only the necessary permissions (e.g., `manage compute-instances` only for operations, `read object-family` for auditors).\n",
      "        *   **Warning:** Avoid granting overly broad permissions like `manage all-resources`.\n",
      "    *   **Instance Principals for Services:** The GPU inference instances should use instance principals to authenticate and authorize against OCI services (e.g., fetching models from Object Storage, publishing logs). This eliminates the need for hardcoded credentials.\n",
      "        ```\n",
      "        # Policy for inference instances to read models from Object Storage and write logs\n",
      "        Allow dynamic-group <your_inference_instance_dynamic_group> to read object-family in compartment <model_compartment> where target.bucket.name = '<your_model_bucket_name>'\n",
      "        Allow dynamic-group <your_inference_instance_dynamic_group> to manage objects in compartment <log_compartment> where target.bucket.name = '<your_log_bucket_name>'\n",
      "        Allow dynamic-group <your_inference_instance_dynamic_group> to use tag-namespaces in compartment <resource_compartment>\n",
      "        ```\n",
      "    *   **API Key Management:** If external systems interact via OCI APIs, manage API keys securely using OCI Vault or limit their scope to specific operations and resources.\n",
      "    *   **Secure CI/CD Identities:** Your CI/CD pipelines (for model updates) must have dedicated, least-privilege identities to access model storage, container registries, and deployment targets.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Network Security**\n",
      "\n",
      "Your real-time, low-latency requirements necessitate a robust and secure network architecture.\n",
      "\n",
      "*   **Concern:** Exposure of inference endpoints, backend compute, or internal communication channels to unauthorized access, DDoS attacks, or data interception.\n",
      "*   **Requirement:** Segment and control network traffic meticulously.\n",
      "\n",
      "    *   **Virtual Cloud Network (VCN):** Design with separate subnets for different tiers:\n",
      "        *   **Public Subnet:** For the Load Balancer endpoint (if externally accessible).\n",
      "        *   **Private Subnets:** For GPU inference instances, model storage access (via Service Gateway), and log storage access.\n",
      "        *   **Warning:** Never place compute instances directly in a public subnet if they contain sensitive data or business logic.\n",
      "    *   **Network Security Groups (NSGs) / Security Lists:**\n",
      "        *   **Inference Instances:** Restrict ingress to *only* the Load Balancer IP range or NSG. Restrict egress to *only* necessary OCI services (Object Storage, Logging) via Service Gateway, patching servers, and potentially model repositories.\n",
      "        *   **Load Balancer:** Allow ingress from specific client IP ranges or the internet (0.0.0.0/0) only on ports 80/443. Egress to backend inference instances.\n",
      "        *   **Policy Example (NSG for Inference Instances):**\n",
      "            ```\n",
      "            # Ingress Rule (from Load Balancer NSG to Inference Instance NSG)\n",
      "            Source Type: NSG\n",
      "            Source NSG: <Load_Balancer_NSG_OCID>\n",
      "            IP Protocol: TCP\n",
      "            Destination Port Range: <Inference_Service_Port> (e.g., 8000)\n",
      "\n",
      "            # Egress Rule (to Object Storage via Service Gateway for models/logs)\n",
      "            Destination Type: Service Gateway\n",
      "            Destination Service: OCI <Region_Identifier> Object Storage\n",
      "            IP Protocol: TCP\n",
      "            Destination Port Range: 443\n",
      "            ```\n",
      "    *   **Web Application Firewall (WAF):** If your inference endpoint is exposed publicly via HTTP/S, a WAF is **non-negotiable**.\n",
      "        *   **WAF Benefits:** Protects against common web exploits (SQL injection, XSS â€“ though less common for ML APIs, still possible), bot protection, API abuse, and DDoS mitigation for HTTP/S traffic. This is critical for the 1000 req/sec peak.\n",
      "        *   **Warning:** Unprotected public API endpoints are frequent targets for attacks.\n",
      "    *   **DDoS Protection:** OCI provides default DDoS protection at the network edge. For advanced L7 protection, the WAF is key.\n",
      "    *   **Service Gateway & Private Endpoints:** Crucial for secure and private communication between your inference instances in private subnets and regional OCI services (Object Storage, Vault, Logging Service) without traversing the public internet.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Data Security & Key Management (Vault)**\n",
      "\n",
      "Protecting your models, inference data, and logs is paramount, especially when handling potentially sensitive requests and intellectual property.\n",
      "\n",
      "*   **Concern:** Unauthorized access, modification, or disclosure of models, input/output data, or logs.\n",
      "*   **Requirement:** Encrypt all data at rest and in transit, and manage encryption keys securely.\n",
      "\n",
      "    *   **Encryption at Rest:**\n",
      "        *   **Models:** Store ML models in OCI Object Storage with **Customer-Managed Keys (CMK)** from OCI Vault, rather than Oracle-managed keys. This gives you full control over the encryption key lifecycle.\n",
      "        *   **Logs:** Inference logs should be stored in Object Storage, also encrypted with CMKs.\n",
      "        *   **Boot & Block Volumes:** Encrypt all boot and block volumes attached to your GPU instances using CMKs.\n",
      "    *   **Encryption in Transit:**\n",
      "        *   **HTTPS/TLS:** Enforce valid TLS certificates on your Load Balancer and between the Load Balancer and inference instances. All client communication with the inference service should use HTTPS.\n",
      "        *   **Internal Service Communication:** Use TLS for any internal service-to-service communication.\n",
      "        *   **Warning:** Using self-signed certificates or unencrypted internal traffic significantly reduces your security posture.\n",
      "    *   **OCI Vault (Key Management):**\n",
      "        *   Centralized management of master encryption keys (CMK used for Object Storage, Block Volume).\n",
      "        *   Store any application-level secrets (e.g., API keys, database credentials) required by your inference service securely in Vault.\n",
      "        *   Implement key rotation policies.\n",
      "        *   Control access to keys and secrets using IAM policies (least privilege).\n",
      "    *   **Data Masking/Anonymization:** If inference input or output contains PII or sensitive data, consider implementing data masking or anonymization techniques *before* logging or storing results, if legally permissible.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Compute & Runtime Security**\n",
      "\n",
      "Securing the GPU-accelerated instances running your inference service.\n",
      "\n",
      "*   **Concern:** Compromise of the compute instances leading to data breaches, unauthorized model execution, or resource misuse.\n",
      "*   **Requirement:** Harden and continuously monitor the compute environment.\n",
      "\n",
      "    *   **Hardened Images:** Start with OCI provided hardened images or create a custom image patched and configured to CIS benchmarks.\n",
      "    *   **Regular Patching:** Implement an automated process for OS, GPU driver, and application patching.\n",
      "    *   **Runtime Protection:**\n",
      "        *   Utilize host-based firewalls (e.g., `iptables` / `firewalld`) on the instances to further restrict traffic, even if NSGs are present.\n",
      "        *   If using containers (e.g., via OKE or Container Instances), scan container images for vulnerabilities *before* deployment and enforce runtime security policies.\n",
      "    *   **Security Zones:** Consider placing your critical GPU inference instances within an OCI Security Zone. This preventative security measure ensures that resources are deployed only if they conform to strong security policies (e.g., no public IP addresses, encrypted resources, protected against accidental public exposure).\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Application and ML Specific Security**\n",
      "\n",
      "Security specifically pertaining to the ML model and its deployment.\n",
      "\n",
      "*   **Concern:** Model tampering, adversarial attacks, API abuse, or insecure deployment processes.\n",
      "*   **Requirement:** Ensure the integrity of models and secure the application API.\n",
      "\n",
      "    *   **Model Integrity & Pedigree:**\n",
      "        *   Store models in a secure"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      " STAGE 2: DevOps Analysis\n",
      "================================================================================\n",
      "As an OCI DevOps Engineer, I've reviewed your requirements for the real-time ML inference service. This is an exciting challenge that leverages many of OCI's strengths in compute, networking, and CI/CD. The key here is achieving high performance, reliability, and automated operations within budget.\n",
      "\n",
      "Here's a breakdown of the key operational requirements, architectural considerations, and automation opportunities, emphasizing repeatability and Infrastructure as Code (IaC).\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Deconstructing Requirements and OCI Service Mapping\n",
      "\n",
      "| Requirement                          | OCI Service/Feature                                   | Operational Implication                              |\n",
      "| :----------------------------------- | :---------------------------------------------------- | :--------------------------------------------------- |\n",
      "| 1. Handle 1000 RPS at peak           | OCI Container"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      " STAGE 3: Architecture Synthesis\n",
      "================================================================================\n",
      "## Real-time ML Inference Service on OCI: Comprehensive Architectural Recommendation\n",
      "\n",
      "### Executive Summary\n",
      "\n",
      "This document outlines a robust, secure, and operationally efficient infrastructure architecture for deploying your real-time ML inference service on Oracle Cloud Infrastructure (OCI). Our recommendation leverages OCI's native services to meet high-performance requirements (1000 RPS, P99 < 100ms latency), GPU acceleration, zero-downtime model updates, and comprehensive logging, all while adhering to strong security principles and DevOps best practices.\n",
      "\n",
      "The core of the architecture will revolve around **OCI Container Engine for Kubernetes (OKE)** running GPU-enabled worker nodes for flexible, scalable inference deployment. It will be fronted by an **OCI Load Balancer** and protected by an **OCI Web Application Firewall (WAF)** for ingress traffic. **OCI Object Storage** will securely house ML models and inference logs, integrated with **OCI Logging** for centralized log management. All infrastructure will be provisioned and managed via **Infrastructure as Code (Terraform)** and automated through **OCI DevOps Service** CI/CD pipelines, ensuring rapid, consistent, and secure deployments.\n",
      "\n",
      "Security is embedded from the ground up, enforcing Zero Trust principles through granular IAM, network segmentation (VCN, NSGs, Service Gateway), data encryption with OCI Vault (CMKs), and continuous monitoring. Cost optimization is addressed through intelligent resource sizing, autoscaling, and tiered storage.\n",
      "\n",
      "This approach provides a highly available, performant, and secure platform that is scalable for future growth, manageable by a small team, and fiscally responsible.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Core Architectural Design\n",
      "\n",
      "#### 1.1. Network Topology\n",
      "\n",
      "*   **Virtual Cloud Network (VCN):** A dedicated VCN will be established in `us-phoenix-1`.\n",
      "*   **Subnets:**\n",
      "    *   **Public Subnet:** Hosts the OCI Load Balancer and potentially the OCI WAF frontend. This subnet will contain no sensitive compute.\n",
      "    *   **Private Subnet (OKE Compute):** Hosts the OKE worker nodes (GPU instances) responsible for inference. This subnet will have no internet ingress and limited egress (only to required OCI services via Service Gateway).\n",
      "    *   **Private Subnet (OKE Control Plane):** Dedicated for OKE's Kubernetes control plane endpoints, also with restricted network access.\n",
      "    *   **Private Subnet (Internal Services/Management):** For potential future internal services or jump hosts, if required.\n",
      "*   **Networking Security Groups (NSGs):** Granular NSGs will strictly control traffic flow:\n",
      "    *   **Load Balancer NSG:** Ingress from WAF/internet (0.0.0.0/0) on HTTPS (443), egress to OKE worker node NSG on the inference service port.\n",
      "    *   **OKE Worker Node NSG:** Ingress *only* from the Load Balancer NSG on the application port. Egress *only* to OCI Object Storage/Logging via Service Gateway, OCI Container Registry, and required OKE control plane components.\n",
      "    *   **OKE Control Plane NSG:** Managed by OKE, but ensuring it only communicates with authorized worker nodes and OCI services.\n",
      "*   **Service Gateway:** Crucial for secure and private communication from OKE worker nodes (in private subnets) to OCI Object Storage, OCI Logging, OCI Vault, and OCI Container Registry, without traversing the public internet. This enhances both security and performance.\n",
      "*   **Web Application Firewall (WAF):** Deployed as an Edge Policy (`oci_waf_network_address_list` and `oci_waf_web_app_firewall`) to protect the public-facing Load Balancer endpoint. This is *non-negotiable* given the 1000 RPS public-facing requirement, preventing DDoS, API abuse, and common web exploits.\n",
      "\n",
      "#### 1.2. Compute & Inference Engine\n",
      "\n",
      "*   **OCI Container Engine for Kubernetes (OKE):** The primary compute platform. OKE provides:\n",
      "    *   **Scalability:** Horizontal Pod Autoscaler (HPA) based on CPU/GPU utilization or custom metrics (e.g., request queue depth) and Cluster Autoscaler (CA) to add/remove GPU worker nodes based on demand.\n",
      "    *   **Availability:** Multi-AD/FD deployment for high availability, automatic self-healing of nodes and pods.\n",
      "    *   **Zero-Downtime Updates:** Kubernetes rolling updates, blue/green deployments, or canary releases for application and model updates via CI/CD.\n",
      "    *   **GPU Integration:** Specific node pools configured with **VM.GPU.A10.1** (or VM.GPU.A100.1 if budget allows and higher performance is needed) shapes. The NVIDIA GPU Operator will be deployed to OKE to manage GPU drivers and runtimes.\n",
      "*   **Containerization:** The ML inference service will be packaged as a Docker container. This container will include the model and inference runtime (e.g., NVIDIA Triton Inference Server, ONNX Runtime, TensorFlow Serving).\n",
      "*   **Ingress Controller:** A Kubernetes Ingress Controller (e.g., NGINX Ingress) managed by OKE can be used internally to route traffic within the cluster.\n",
      "*   **OCI Load Balancer:**\n",
      "    *   **Type:** Flexible Load Balancer (Public for WAF integration, Private for internal if needed).\n",
      "    *   **Configuration:** Health checks (HTTP/TCP) to OKE pods. SSL termination at the Load Balancer using certificates stored in OCI Vault.\n",
      "    *   **Performance:** Capable of handling high throughput; ensure adequate bandwidth.\n",
      "\n",
      "#### 1.3. Data Storage & Management\n",
      "\n",
      "*   **OCI Object Storage:**\n",
      "    *   **Model Storage:** A dedicated bucket for storing ML models (ONNX, SavedModel, etc.).\n",
      "    *   **Inference Log Storage:** A dedicated bucket for long-term storage of inference logs, integrated with OCI Logging.\n",
      "    *   **Security:** Both buckets will be encrypted using **Customer-Managed Keys (CMKs)** from **OCI Vault**. Access will be restricted via IAM policies and bucket policies.\n",
      "    *   **Lifecycle Management:** For logs, configure Object Storage lifecycle policies to move logs to lower-cost archive storage after a defined period (e.g., 30 days) and delete them after 90 days.\n",
      "\n",
      "#### 1.4. Monitoring, Logging & Observability\n",
      "\n",
      "*   **OCI Logging Service:** All inference service logs (application, access logs) will be structured (e.g., JSON) and directed to OCI Logging via `fluentd` or `fluentbit` within OKE.\n",
      "    *   **Log Groups & Unified Agents:** Create a dedicated log group and leverage the OCI Logging Unified Agent for collection.\n",
      "    *   **Archiving:** Configure an OCI Logging connector to automatically export logs from OCI Logging to the designated Object Storage bucket for 90-day retention.\n",
      "*   **OCI Monitoring Service:**\n",
      "    *   **Infrastructure Metrics:** Monitor OKE node health (CPU, memory, disk, network, GPU utilization), Load Balancer metrics (request count, latency, error rates).\n",
      "    *   **Application Metrics:** Collect custom metrics from the inference service (e.g., inference latency, RPS, model load times, error rates) using Prometheus and Grafana (deployed within OKE) or pushing directly to OCI Monitoring.\n",
      "    *   **Alarms:** Configure alarms for critical thresholds (e.g., high latency, low GPU memory, increased error rates, autoscaling events) and integrate with OCI Notifications (email, PagerDuty, Slack).\n",
      "*   **OCI Application Performance Monitoring (APM):** For deeper tracing and performance analysis of the inference service, particularly useful if the service interacts with other microservices.\n",
      "\n",
      "### 2. Security Integration (Zero Trust Foundation)\n",
      "\n",
      "The security analysis highlights critical considerations, which are integrated as follows:\n",
      "\n",
      "*   **Identity and Access Management (IAM):**\n",
      "    *   **Least Privilege:** All policies will follow the principle of least privilege.\n",
      "    *   **Dynamic Groups:** OKE worker nodes will be members of dynamic groups, enabling them to use **Instance Principals** to authenticate with OCI services (Object Storage, Logging, Vault) without API Keys.\n",
      "    *   **Dedicated IAM Groups:** For ML engineers, MLOps, security auditors, with specific, limited permissions.\n",
      "    *   **Secure CI/CD Identities:** CI/CD pipelines (via OCI DevOps Service) will use dedicated principal identities with scoped permissions to OCI Container Registry, OKE, and Object Storage.\n",
      "*   **Network Security:**\n",
      "    *   **VCN Segmentation:** As detailed above, with private subnets for all sensitive compute.\n",
      "    *   **NSGs:** Strictly enforced ingress/egress rules for all components.\n",
      "    *   **WAF:** Mandatory for public-facing endpoint DDoS and L7 protection.\n",
      "    *   **Service Gateway:** Essential for private access to OCI services.\n",
      "*   **Data Security & Key Management (OCI Vault):**\n",
      "    *   **Everything Encrypted with CMKs:**\n",
      "        *   Models in Object Storage.\n",
      "        *   Inference logs in Object Storage.\n",
      "        *   Boot and block volumes of OKE worker nodes.\n",
      "    *   **TLS Everywhere:** HTTPS for client-LB, LB-OKE, and any internal service-to-service communication.\n",
      "    *   **OCI Vault for Secrets:** All application secrets, API keys, and TLS certificates will be managed in OCI Vault. OKE pods will retrieve secrets dynamically.\n",
      "*   **Compute & Runtime Security:**\n",
      "    *   **Hardened Images:** OKE node pools will use OCI-provided hardened images, with automated patching.\n",
      "    *   **Container Security:** OCI Container Registry scanning for vulnerabilities on push. Runtime security policies (e.g., using Kubernetes Network Policies, OCI Security Posture Management) to limit container capabilities and network access.\n",
      "    *   **Security Zones:** Consider placing the OKE private subnets and worker nodes within an OCI Security Zone to enforce preventive security policies (e.g., no public IPs, encrypted resources by default).\n",
      "*   **Application and ML Specific Security:**\n",
      "    *   **Model Integrity:** CI/CD pipeline will verify model source and integrity before deployment.\n",
      "    *   **Input Validation:** Application-level validation of inference request inputs to prevent adversarial attacks or malformed requests.\n",
      "    *   **Rate Limiting:** Implemented at the WAF and/or Load Balancer level to prevent API abuse.\n",
      "\n",
      "### 3. DevOps Integration & Implementation Plan\n",
      "\n",
      "The DevOps analysis provides excellent opportunities for automation and efficiency.\n",
      "\n",
      "*   **Infrastructure as Code (IaC) with Terraform:**\n",
      "    *   All OCI resources (VCN, Subnets, NSGs, OKE Cluster, Node Pools, Load Balancer, WAF, Object Storage, IAM Policies, Vault, Logging configurations) will be defined and managed using Terraform.\n",
      "    *   This ensures repeatability, version control, and auditability of the entire infrastructure.\n",
      "*   **CI/CD Pipeline with OCI DevOps Service:**\n",
      "    *   **Model Training and Packaging:** When a new ML model is trained and validated, it's packaged (e.g., within the inference container image or as a separate artifact).\n",
      "    *   **Commit:** Changes (code, model artifacts, Kubernetes manifests) are committed to an OCI Code Repository (or GitHub/GitLab).\n",
      "    *   **Build Stage:**\n",
      "        *   Triggered on commit.\n",
      "        *   Builds the Docker container image for the inference service.\n",
      "        *   Performs vulnerability scanning on the Docker image (e.g., using OCI Container Registry scanning/Trivy).\n",
      "        *   Pushes the image to OCI Container Registry.\n",
      "        *   Updates Kubernetes deployment manifests (e.g., image tag, model version).\n",
      "    *   **Deployment Stage:**\n",
      "        *   Triggers after successful build.\n",
      "        *   Deploys changes to OKE using kubectl configured via OCI DevOps stages.\n",
      "        *   **Deployment Strategy:** Blue/Green or Rolling Update for zero-downtime model updates.\n",
      "            *   **Rolling Update (default):** Gradually replaces old pods with new ones. Health checks ensure new pods are ready before decommissioning old ones.\n",
      "            *   **Blue/Green:** Deploys a new \"green\" version alongside the existing \"blue\" version. Traffic is then shifted via Load Balancer/Ingress Controller. This is preferred for critical services and requires managing two identical sets of resources temporarily.\n",
      "        *   Post-deployment automated smoke tests and health checks.\n",
      "*   **Observability:** Integrated monitoring and logging from day zero to quickly detect and resolve issues. Alarms notify relevant teams via OCI Notifications.\n",
      "\n",
      "#### 3.1. Implementation Plan\n",
      "\n",
      "**Phase 1: Foundation (Estimated: 2 weeks)**\n",
      "\n",
      "1.  **IAM Zero Trust Baseline:**\n",
      "    *   Define compartments for Network, Compute, Data, Security, DevOps.\n",
      "    *   Create IAM groups (Admin, Networking, OKE Admin, ML Engineer, MLOps, Security Auditor).\n",
      "    *   Define initial, least-privilege IAM policies, including dynamic groups for OKE node instance principals.\n",
      "2.  **Network Setup (Terraform):**\n",
      "    *   Design and provision VCN, public, private subnets.\n",
      "    *   Configure Security Lists and NSGs for Load Balancer and OKE worker nodes.\n",
      "    *   Deploy Service Gateway for private OCI service access.\n",
      "3.  **Security Core Services (Terraform):**\n",
      "    *   Provision OCI Vault, create CMKs.\n",
      "    *   Provision OCI Object Storage buckets (models, logs) configured with CMKs.\n",
      "    *   Initial OCI\n",
      "\n",
      "\n",
      "================================================================================\n",
      "âœ… REVIEW COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the infrastructure review workflow\n",
    "requirements = \"\"\"\n",
    "We need to deploy a real-time ML inference service with the following requirements:\n",
    "\n",
    "1. Handle 1000 requests per second at peak\n",
    "2. Maximum latency of 100ms for 99th percentile\n",
    "3. Use GPU-accelerated inference\n",
    "4. Support model updates without downtime\n",
    "5. Store inference logs for 90 days\n",
    "6. Operate in us-phoenix-1 region\n",
    "7. Budget: ~$10,000/month\n",
    "\"\"\"\n",
    "\n",
    "review_results = infrastructure_review_workflow(requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f53132",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Cleanup\n",
    "\n",
    "Clean up all sessions created during this notebook run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d26aca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: DELETE http://localhost:8321/v1/conversations/conv_b4f4097126e0759eac387b6e898e487d3490f6654381933b \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://localhost:8321/v1/conversations/conv_0ebd02430a7d71a10ef063c25ad60f68e2ff0a2510d4b4e3 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://localhost:8321/v1/conversations/conv_ffd7ea8c7cce7d2fbe76b96bda4d38c45809e189178d2e23 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://localhost:8321/v1/conversations/conv_a54387c8c21f9f1f05d030695c5256f359cd8847a46f6abb \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://localhost:8321/v1/conversations/conv_c85584e0c7fc03a4be822479d0d96805bd46e311ef4cf9af \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://localhost:8321/v1/conversations/conv_db43f1c47d76959cb34de94922a08ab80167240a01977be5 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://localhost:8321/v1/conversations/conv_1c3d69c80ea5a4a4ce613b09708a9c7a582f595e2f00702b \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://localhost:8321/v1/conversations/conv_a23953cebc92f017349cf569b9537e1f6ca84357b8c465d6 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Total sessions created: 8\n",
      "ðŸ§¹ Cleaning up 8 session(s)...\n",
      "   âœ… Deleted session: conv_b4f4097126e0759eac387b6e898e487d3490f6654381933b\n",
      "   âœ… Deleted session: conv_0ebd02430a7d71a10ef063c25ad60f68e2ff0a2510d4b4e3\n",
      "   âœ… Deleted session: conv_ffd7ea8c7cce7d2fbe76b96bda4d38c45809e189178d2e23\n",
      "   âœ… Deleted session: conv_a54387c8c21f9f1f05d030695c5256f359cd8847a46f6abb\n",
      "   âœ… Deleted session: conv_c85584e0c7fc03a4be822479d0d96805bd46e311ef4cf9af\n",
      "   âœ… Deleted session: conv_db43f1c47d76959cb34de94922a08ab80167240a01977be5\n",
      "   âœ… Deleted session: conv_1c3d69c80ea5a4a4ce613b09708a9c7a582f595e2f00702b\n",
      "   âœ… Deleted session: conv_a23953cebc92f017349cf569b9537e1f6ca84357b8c465d6\n",
      "âœ… Session cleanup complete\n"
     ]
    }
   ],
   "source": [
    "# Clean up all tracked sessions\n",
    "print(f\"ðŸ“Š Total sessions created: {len(all_sessions)}\")\n",
    "cleanup_sessions(all_sessions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
